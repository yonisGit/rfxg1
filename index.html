<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations.">
  <meta name="keywords" content="XAI, Saliency Maps, Interpretability, Computer Vision, RFxG, AAAI 2026">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RFxG: Rethinking Saliency Maps</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/yonisGit/RFxG">
      <span class="icon">
          <i class="fab fa-github"></i>
      </span>
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Rethinking Saliency Maps:<br>A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.co.il/citations?user=gjZHHkAAAAAJ&hl=en">Yehonatan Elisha</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=gv_yHeoAAAAJ&hl=en">Seffi Cohen</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.co.il/citations?hl=en&user=gLs4d6oAAAAJ&view_op=list_works&sortby=pubdate">Oren Barkan</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.co.il/citations?user=RWaXcd0AAAAJ&hl=en">Noam Koenigstein</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tel Aviv University,</span>
            <span class="author-block"><sup>2</sup>Harvard University,</span>
            <span class="author-block"><sup>3</sup>The Open University</span>
            <br><b>AAAI 2026</b>
          </div>
          
          <!-- <div class="is-size-5 publication-venue">
            <span class="author-block"><b>AAAI 2026</b></span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2511.13081v2"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2511.13081"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/yonisGit/RFxG"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.png" alt="RFxG Taxonomy Axes" style="width:100%; height:auto;">
      <h2 class="subtitle has-text-centered">
        The <strong>Reference-Frame &times; Granularity (RFxG)</strong> Taxonomy. <br>
        We conceptualize saliency explanations along two essential axes: 
        the <i>reference-frame</i> (pointwise vs. contrastive) and the <i>granularity</i> (fine-grained class vs. coarse-grained group).
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Saliency maps have become a cornerstone of visual explanation in deep learning, yet there remains no consensus on their intended purpose and their alignment with specific user queries. This fundamental ambiguity undermines both the evaluation and practical utility of explanation methods. 
          </p>
          <p>
            In this paper, we introduce the <strong>Reference-Frame &times; Granularity (RFxG)</strong> taxonomyâ€”a principled framework that addresses this ambiguity by conceptualizing saliency explanations along two essential axes: the <strong>reference-frame axis</strong> (distinguishing between pointwise "Why Husky?" and contrastive "Why Husky and not Shih-tzu?" explanations) and the <strong>granularity axis</strong> (ranging from fine-grained class-level to coarse-grained group-level interpretations, e.g., "Why Husky?" vs. "Why Dog?").
          </p>
          <p>
            Through this lens, we identify critical limitations in existing evaluation metrics, which predominantly focus on pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To address these gaps, we propose four novel faithfulness metrics that systematically assess explanation quality across both RFxG dimensions: <strong>CCS, CGC, PGS, and CGS</strong>. 
          </p>
          <p>
            Our comprehensive evaluation framework spans ten state-of-the-art methods, 4 model architectures, and 3 datasets. By suggesting a shift from model-centric to user-intent-driven evaluation, our work provides both the conceptual foundation and practical tools necessary for developing explanations that are not only faithful to model behavior but also meaningfully aligned with human understanding.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Qualitative Results</h2>
        <div class="content has-text-justified">
          <p>
            Different user queries require different types of saliency maps. Our taxonomy helps distinguish between these intent-driven explanations.
          </p>
        </div>
        
        <div class="columns is-centered">
          <div class="column is-10 has-text-centered">
             <img src="./static/images/visual_example.png" alt="Qualitative comparison of saliency maps" style="width:100%; border-radius: 10px;">
            <p class="caption">
              <strong>Saliency maps for a sports car answering different questions:</strong><br>
              (a) Pointwise class: "Why sports car?"<br>
              (b) Contrastive between two classes: "Why sports car and not a convertible?"<br>
              (c) Contrastive between class and group: "Why sports car and not other cars?"
            </p>
          </div>
        </div>

        <h3 class="title is-4">Key Findings</h3>
        <div class="content has-text-justified">
          <ul>
            <li>
              <strong>Ambiguity in Saliency:</strong> Without explicit context about reference frame or granularity, users often misinterpret saliency maps.
            </li>
            <li>
              <strong>Metric Gaps:</strong> Existing metrics focus on pointwise faithfulness. They fail to capture whether an explanation distinguishes between classes (e.g., Husky vs. Shih-tzu).
            </li>
            <li>
              <strong>IIA Superiority:</strong> The <strong>Iterated Integrated Attributions (IIA)</strong> method consistently outperforms others on contrastive and group-level metrics, effectively capturing multi-level features.
            </li>
          </ul>
        </div>

      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{elisha2026rethinking,
  title     = {Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations},
  author    = {Elisha, Yehonatan and Cohen, Seffi and Barkan, Oren and Koenigstein, Noam},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year      = {2026}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2511.13081v2">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/yonisGit/RFxG" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This page was built using the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies website template</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
